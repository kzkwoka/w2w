{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/net/tscratch/people/plgkingak'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"weights2weights/weights_datasets\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dimensions = torch.load(f\"{path}/weight_dimensions.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_df = pd.DataFrame(weight_dimensions).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(1, 320)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(320, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(1, 320)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(320, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(1, 320)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.lora_B.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(320, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_A.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(1, 320)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.lora_B.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(320, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_A.weight</th>\n",
       "      <td>(768,)</td>\n",
       "      <td>(1, 768)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.lora_B.weight</th>\n",
       "      <td>(320,)</td>\n",
       "      <td>(320, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0         1\n",
       "base_model.model.down_blocks.0.attentions.0.tra...  (320,)  (1, 320)\n",
       "base_model.model.down_blocks.0.attentions.0.tra...  (320,)  (320, 1)\n",
       "base_model.model.down_blocks.0.attentions.0.tra...  (320,)  (1, 320)\n",
       "base_model.model.down_blocks.0.attentions.0.tra...  (320,)  (320, 1)\n",
       "base_model.model.down_blocks.0.attentions.0.tra...  (320,)  (1, 320)\n",
       "...                                                    ...       ...\n",
       "base_model.model.up_blocks.3.attentions.2.trans...  (320,)  (320, 1)\n",
       "base_model.model.up_blocks.3.attentions.2.trans...  (320,)  (1, 320)\n",
       "base_model.model.up_blocks.3.attentions.2.trans...  (320,)  (320, 1)\n",
       "base_model.model.up_blocks.3.attentions.2.trans...  (768,)  (1, 768)\n",
       "base_model.model.up_blocks.3.attentions.2.trans...  (320,)  (320, 1)\n",
       "\n",
       "[128 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_df[\"use\"] = 0\n",
    "dim_df.loc[dim_df.index.str.contains(\"mid_block\"), \"use\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_df.use.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_df[\"lengths\"] = [a.numel() for a,_ in list(weight_dimensions.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_df[\"start\"] = dim_df.lengths.cumsum().shift(fill_value=0)\n",
    "dim_df[\"end\"] = dim_df.start + dim_df.lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>use</th>\n",
       "      <th>lengths</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_A.weight</th>\n",
       "      <td>(1280,)</td>\n",
       "      <td>(1, 1280)</td>\n",
       "      <td>1</td>\n",
       "      <td>1280</td>\n",
       "      <td>35968</td>\n",
       "      <td>37248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.lora_B.weight</th>\n",
       "      <td>(1280,)</td>\n",
       "      <td>(1280, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>1280</td>\n",
       "      <td>37248</td>\n",
       "      <td>38528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_A.weight</th>\n",
       "      <td>(1280,)</td>\n",
       "      <td>(1, 1280)</td>\n",
       "      <td>1</td>\n",
       "      <td>1280</td>\n",
       "      <td>38528</td>\n",
       "      <td>39808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.lora_B.weight</th>\n",
       "      <td>(1280,)</td>\n",
       "      <td>(1280, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>1280</td>\n",
       "      <td>39808</td>\n",
       "      <td>41088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_A.weight</th>\n",
       "      <td>(1280,)</td>\n",
       "      <td>(1, 1280)</td>\n",
       "      <td>1</td>\n",
       "      <td>1280</td>\n",
       "      <td>41088</td>\n",
       "      <td>42368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.lora_B.weight</th>\n",
       "      <td>(1280,)</td>\n",
       "      <td>(1280, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>1280</td>\n",
       "      <td>42368</td>\n",
       "      <td>43648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_A.weight</th>\n",
       "      <td>(768,)</td>\n",
       "      <td>(1, 768)</td>\n",
       "      <td>1</td>\n",
       "      <td>768</td>\n",
       "      <td>43648</td>\n",
       "      <td>44416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_model.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.lora_B.weight</th>\n",
       "      <td>(1280,)</td>\n",
       "      <td>(1280, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>1280</td>\n",
       "      <td>44416</td>\n",
       "      <td>45696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0          1  use  \\\n",
       "base_model.model.mid_block.attentions.0.transfo...  (1280,)  (1, 1280)    1   \n",
       "base_model.model.mid_block.attentions.0.transfo...  (1280,)  (1280, 1)    1   \n",
       "base_model.model.mid_block.attentions.0.transfo...  (1280,)  (1, 1280)    1   \n",
       "base_model.model.mid_block.attentions.0.transfo...  (1280,)  (1280, 1)    1   \n",
       "base_model.model.mid_block.attentions.0.transfo...  (1280,)  (1, 1280)    1   \n",
       "base_model.model.mid_block.attentions.0.transfo...  (1280,)  (1280, 1)    1   \n",
       "base_model.model.mid_block.attentions.0.transfo...   (768,)   (1, 768)    1   \n",
       "base_model.model.mid_block.attentions.0.transfo...  (1280,)  (1280, 1)    1   \n",
       "\n",
       "                                                    lengths  start    end  \n",
       "base_model.model.mid_block.attentions.0.transfo...     1280  35968  37248  \n",
       "base_model.model.mid_block.attentions.0.transfo...     1280  37248  38528  \n",
       "base_model.model.mid_block.attentions.0.transfo...     1280  38528  39808  \n",
       "base_model.model.mid_block.attentions.0.transfo...     1280  39808  41088  \n",
       "base_model.model.mid_block.attentions.0.transfo...     1280  41088  42368  \n",
       "base_model.model.mid_block.attentions.0.transfo...     1280  42368  43648  \n",
       "base_model.model.mid_block.attentions.0.transfo...      768  43648  44416  \n",
       "base_model.model.mid_block.attentions.0.transfo...     1280  44416  45696  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_df[dim_df.use == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9728"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.concatenate([np.arange(row.start, row.end) for _, row in dim_df[dim_df.use == 1].iterrows()])\n",
    "len(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(f\"{path}/identities/all_weights.pt\", torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(weights[:,indices], f'{path}/mid_block.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(f'{path}/mid_block.pt', torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629e4adce756427db8ea32f341c93a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/64974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Dataset.from_dict({\"data\":dataset}).save_to_disk(f'{path}/mid_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset.from_dict({\"data\":dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['data'],\n",
       "    num_rows: 64974\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9728"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d[0][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan  8 17:11:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:8D:00.0 Off |                    0 |\n",
      "| N/A   27C    P0             73W /  400W |   25121MiB /  40960MiB |     39%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1493243      C   ...tch/people/plgkingak/w2w/bin/python      25114MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dad0ff92d44291aff38c99fe676f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 18.3\n",
      "RAM Used (GB): 169.273266176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b8489a0126453fb83d91d1e6d001c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 18.3\n",
      "RAM Used (GB): 168.787308544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f90f60a15c4553adc0833952e5ee25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 18.2\n",
      "RAM Used (GB): 169.674145792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a09f02536b14d2482846482a6806d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 18.3\n",
      "RAM Used (GB): 168.61114368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1374243ef443288f037818c5609efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 18.3\n",
      "RAM Used (GB): 169.329750016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78199774b9114f8dbde602f85eef82a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 18.4\n",
      "RAM Used (GB): 169.76019456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf8f3f5f8194e078ccf202e5707732f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/4974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory % used: 18.1\n",
      "RAM Used (GB): 167.055147008\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "for i, chunk in enumerate(weights.split(10000)):\n",
    "    dc = Dataset.from_dict({\"data\":chunk})\n",
    "    dc.save_to_disk(f'{path}/full/{i}')\n",
    "    print('RAM memory % used:', psutil.virtual_memory()[2])\n",
    "    print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
